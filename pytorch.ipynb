{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to Data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to Data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to Data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to Data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to Data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to Data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to Data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to Data\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"Data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"Data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N,C,H,W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X,y in test_dataloader:\n",
    "    print(\"Shape of X [N,C,H,W]:\",X.shape)\n",
    "    print(\"Shape of y:\",y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pas de fou compris mais en gros taille 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork,self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=1e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        X,y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred,y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch%100 == 0:\n",
    "            loss, current = loss.item(), (batch+1)*len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred,y).item()\n",
    "            correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.310750 [   64/60000]\n",
      "loss: 2.301026 [ 6464/60000]\n",
      "loss: 2.295597 [12864/60000]\n",
      "loss: 2.293096 [19264/60000]\n",
      "loss: 2.275095 [25664/60000]\n",
      "loss: 2.259380 [32064/60000]\n",
      "loss: 2.271753 [38464/60000]\n",
      "loss: 2.252720 [44864/60000]\n",
      "loss: 2.250190 [51264/60000]\n",
      "loss: 2.232636 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 2177.1%, Avg loss: 2.230599 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.246064 [   64/60000]\n",
      "loss: 2.238172 [ 6464/60000]\n",
      "loss: 2.216424 [12864/60000]\n",
      "loss: 2.218455 [19264/60000]\n",
      "loss: 2.160670 [25664/60000]\n",
      "loss: 2.146877 [32064/60000]\n",
      "loss: 2.161301 [38464/60000]\n",
      "loss: 2.128440 [44864/60000]\n",
      "loss: 2.145337 [51264/60000]\n",
      "loss: 2.085400 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 3245.2%, Avg loss: 2.097841 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.150029 [   64/60000]\n",
      "loss: 2.127103 [ 6464/60000]\n",
      "loss: 2.087420 [12864/60000]\n",
      "loss: 2.082872 [19264/60000]\n",
      "loss: 1.958241 [25664/60000]\n",
      "loss: 1.965211 [32064/60000]\n",
      "loss: 1.975884 [38464/60000]\n",
      "loss: 1.927373 [44864/60000]\n",
      "loss: 1.984436 [51264/60000]\n",
      "loss: 1.848998 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 3468.8%, Avg loss: 1.890084 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.012030 [   64/60000]\n",
      "loss: 1.954320 [ 6464/60000]\n",
      "loss: 1.902193 [12864/60000]\n",
      "loss: 1.876033 [19264/60000]\n",
      "loss: 1.684243 [25664/60000]\n",
      "loss: 1.742620 [32064/60000]\n",
      "loss: 1.736230 [38464/60000]\n",
      "loss: 1.698799 [44864/60000]\n",
      "loss: 1.796903 [51264/60000]\n",
      "loss: 1.587983 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 3637.6%, Avg loss: 1.670160 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.867837 [   64/60000]\n",
      "loss: 1.782702 [ 6464/60000]\n",
      "loss: 1.732088 [12864/60000]\n",
      "loss: 1.675370 [19264/60000]\n",
      "loss: 1.450098 [25664/60000]\n",
      "loss: 1.563477 [32064/60000]\n",
      "loss: 1.529611 [38464/60000]\n",
      "loss: 1.525575 [44864/60000]\n",
      "loss: 1.633702 [51264/60000]\n",
      "loss: 1.394078 [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 3701.9%, Avg loss: 1.501149 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader,model,loss_fn,optimizer)\n",
    "    test(test_dataloader,model,loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save / load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(),\"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle Boot\", Actual: \"Ankle Boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle Boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x,y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TENSORS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from random data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "data = [[1,2],[3,4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from another tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.7165, 0.0670],\n",
      "        [0.7727, 0.2409]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with random / constant val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.6081, 0.0163, 0.7000],\n",
      "        [0.3705, 0.2430, 0.4944]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute of a Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test operation (https://pytorch.org/docs/stable/torch.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7996, 0.6553, 0.0637, 0.9744],\n",
      "        [0.8335, 0.2038, 0.8259, 0.2494],\n",
      "        [0.7871, 0.0456, 0.8692, 0.2117]])\n",
      "tensor([[0.7996, 0.8335, 0.7871],\n",
      "        [0.6553, 0.2038, 0.0456],\n",
      "        [0.0637, 0.8259, 0.8692],\n",
      "        [0.9744, 0.2494, 0.2117]])\n",
      "tensor(0.2038) tensor([0.8335, 0.2038, 0.8259, 0.2494]) tensor([0.6553, 0.2038, 0.0456])\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')\n",
    "\n",
    "print(tensor)\n",
    "print(tensor.adjoint())\n",
    "\n",
    "print(\n",
    "    tensor[1][1],\n",
    "tensor[1,:],\n",
    "tensor[:,1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np / tensor exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## datasets & dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to understand :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on use un dataloader qui permet de faire des transformation sur les data, redistribuer etc de façon facile et multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    }
   ],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "totensor => PIL image or ndarray to floattensor.\n",
    "Lambda transform => user lambda function\n",
    "https://pytorch.org/vision/stable/transforms.html\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "##Chose the good device\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we devine the nn class using nn :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([1])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a try with minibatch of 3 image of 28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do only 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.0045,  0.1200, -0.1361,  0.0687,  0.0949,  0.1400, -0.0066,  0.0367,\n",
      "         -0.1696, -0.0258, -0.3087, -0.1778, -0.1704,  0.4306, -0.2328, -0.0240,\n",
      "          0.3738,  0.2243, -0.6334, -0.0069],\n",
      "        [-0.0173,  0.3446,  0.0974,  0.2446,  0.3338,  0.4544,  0.0885,  0.1563,\n",
      "         -0.1180, -0.1179, -0.5783, -0.0975, -0.5412,  0.0466, -0.3874, -0.2040,\n",
      "          0.5517,  0.0482, -0.3922, -0.2541],\n",
      "        [-0.0268,  0.2150,  0.1843, -0.1385,  0.1439,  0.5906, -0.1024, -0.0040,\n",
      "         -0.2703,  0.0775, -0.4632,  0.0124, -0.4929,  0.4290, -0.1366,  0.0102,\n",
      "          0.3248,  0.0232, -0.3479, -0.1237]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0045, 0.1200, 0.0000, 0.0687, 0.0949, 0.1400, 0.0000, 0.0367, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.4306, 0.0000, 0.0000, 0.3738, 0.2243,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.3446, 0.0974, 0.2446, 0.3338, 0.4544, 0.0885, 0.1563, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0466, 0.0000, 0.0000, 0.5517, 0.0482,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.2150, 0.1843, 0.0000, 0.1439, 0.5906, 0.0000, 0.0000, 0.0000,\n",
      "         0.0775, 0.0000, 0.0124, 0.0000, 0.4290, 0.0000, 0.0102, 0.3248, 0.0232,\n",
      "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pas trop compris non linéarité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "permet de refaire pareil dans un sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "permet de transformer en proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0131, -0.0265,  0.0029,  ...,  0.0097, -0.0184,  0.0149],\n",
      "        [-0.0159,  0.0335, -0.0121,  ...,  0.0113,  0.0072, -0.0316]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0052, -0.0146], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0051,  0.0285, -0.0155,  ...,  0.0214, -0.0400,  0.0004],\n",
      "        [-0.0204,  0.0353,  0.0080,  ..., -0.0178,  0.0335,  0.0317]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0238,  0.0027], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0028, -0.0405, -0.0341,  ...,  0.0259,  0.0202,  0.0285],\n",
      "        [-0.0063,  0.0062,  0.0066,  ...,  0.0397, -0.0294,  0.0387]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([0.0054, 0.0047], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une façon d'acceder au paramettres"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUTOMATIC DIFFERENTIATION WITH TORCH.AUTOGRAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute les gradient :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3317, 0.0015, 0.2054],\n",
      "        [0.3317, 0.0015, 0.2054],\n",
      "        [0.3317, 0.0015, 0.2054],\n",
      "        [0.3317, 0.0015, 0.2054],\n",
      "        [0.3317, 0.0015, 0.2054]])\n",
      "tensor([0.3317, 0.0015, 0.2054])\n"
     ]
    }
   ],
   "source": [
    "loss.backward() #work only if required_grad = true before\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en gros si on veux pas du grad on fait un with ou detach : \n",
    "\n",
    "To mark some parameters in your neural network as frozen parameters.\n",
    "\n",
    "To speed up computations when you are only doing forward pass, because computations on tensors that do not track gradients would be more efficient."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aussi un truc aavec les matrices jacobienne"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIMIZING MODEL PARAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimize qui fait qq truc :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on augmente les loops (epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.303688  [   64/60000]\n",
      "loss: 2.292185  [ 6464/60000]\n",
      "loss: 2.275231  [12864/60000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m     train_loop(train_dataloader, model, loss_fn, optimizer)\n\u001b[0;32m      8\u001b[0m     test_loop(test_dataloader, model, loss_fn)\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[82], line 3\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_loop\u001b[39m(dataloader, model, loss_fn, optimizer):\n\u001b[0;32m      2\u001b[0m     size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataloader\u001b[39m.\u001b[39mdataset)\n\u001b[1;32m----> 3\u001b[0m     \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m      4\u001b[0m         \u001b[39m# Compute prediction and loss\u001b[39;00m\n\u001b[0;32m      5\u001b[0m         pred \u001b[39m=\u001b[39m model(X)\n\u001b[0;32m      6\u001b[0m         loss \u001b[39m=\u001b[39m loss_fn(pred, y)\n",
      "File \u001b[1;32mc:\\Users\\Yanis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Yanis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Yanis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Yanis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Yanis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\Yanis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[1;32mc:\\Users\\Yanis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:173\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    172\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m--> 173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39;49m(img, torch\u001b[39m.\u001b[39;49mByteTensor):\n\u001b[0;32m    174\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mdefault_float_dtype)\u001b[39m.\u001b[39mdiv(\u001b[39m255\u001b[39m)\n\u001b[0;32m    175\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
